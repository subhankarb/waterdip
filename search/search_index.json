{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Waterdip Documentation","text":"<p>Welcome to the Waterdip Documentation!</p> <p>Let's jump to the quick start!</p>"},{"location":"concepts/metric_type_data_quality/","title":"Data Quality Metrics","text":"<p>ML Model health depends upon high-quality data features. Data quality metrics help identify key quality issues such as data type mismatches, missing data, and more.</p>"},{"location":"concepts/metric_type_data_quality/#list-of-data-quality-metrics","title":"List of Data Quality Metrics","text":"Metrics Categorical Numeric Boolean Description % of empty values The percent of nulls in model features Missing Values Count of new unique values that appear in baseline but not in production New Values Count of new unique values that appear in production but not in baseline"},{"location":"concepts/metric_type_drift/","title":"Drift Metrics","text":"<p>The term \"data drift\" describes a phenomenon whereby data gets increasingly inaccurate as it ages. Distribution drift, which is the difference between two statistical distributions. The first distribution is the historic record of known information. The second is a forecast using that historic record. Data drift can have a negative effect on forecasts and other statistical analyses.</p> <p>Data drift occurs because of external factors that cause changes to the distribution of data, such as new information from experts or customers, or changes in the method used to collect data. Drift can also be caused by incorrect formulas used in statistical models.</p> <p>The difference between the two distributions is data drift</p>"},{"location":"concepts/metric_type_drift/#list-of-drift-metrics","title":"List of Drift Metrics","text":"Metrics Categorical Numeric Boolean Description PSI Population Stability Index (PSI) compares the distribution of predicted probability in scoring data with predicted probability in training data. The idea is to check \u201cHow different the current scored data is, compared to the training data\u201d. A generic rule to decide on model retraining based on PSI \u2014  1. PSI &lt; 0.1 \u2014 No change. You can continue using existing model.  2. PSI &gt;=0.1 but less than 0.2 \u2014 Slight change is required.  3. PSI &gt;=0.2 \u2014 Significant change is required. Ideally, you should not use this model anymore, retraining is required."},{"location":"concepts/metric_type_model_performance/","title":"Performance Metrics","text":"<p>Performance metrics are measurements that quantitatively calculate your model\u2019s performance. The metrics consider what the model predicted (prediction) against what actually happens (Actuals).</p>"},{"location":"concepts/metric_type_model_performance/#list-of-model-performance-metrics","title":"List of Model Performance Metrics","text":"Metrics Model Family Description Accuracy Classification The base metric used for model evaluation is often Accuracy, describing the number of correct predictions over all predictions Precision Classification Precision is a measure of how many of the positive predictions made are correct (true positives) Recall / Sensitivity Classification Recall is a measure of how many of the positive cases the classifier correctly predicted, over all the positive cases in the data F_1 Classification F1-Score is a measure combining both precision and recall. It is generally described as the harmonic mean of the two Specificity Classification Specificity is a measure of how many negative predictions made are correct (true negatives)"},{"location":"concepts/model_and_versions/","title":"Model And Versions","text":""},{"location":"concepts/monitor_and_alerts/","title":"Monitor &amp; Alerts","text":""},{"location":"concepts/what_is_ml_observability/","title":"What is ML Observability?","text":"<p>ML observability is the ability to measure and report on the performance of machine learning models in real time. It enables organizations to improve model accuracy and reliability by measuring service quality continuously across pre-production and production phases of model life cycles.</p>"},{"location":"concepts/what_is_ml_observability/#what-could-possibly-go-wrong","title":"What could possibly go wrong?","text":""},{"location":"concepts/what_is_ml_observability/#data-distribution","title":"Data Distribution","text":"<p>In machine learning, the most basic assumption is that the distribution of data your model is exposed to changes over time.   For example, if you are working on an image recognition task and have been collecting images of cats for a few months,   and then suddenly start collecting images of dogs, it would be unwise to keep using the same model architecture. A model that has been trained on cats may not work very well on dogs   \u2013 even if there are some visual similarities between cats and dogs (big/pointy ears, tails, etc.).</p> <p> </p> Training Serving Data Distribution"},{"location":"concepts/what_is_ml_observability/#training-serving-skew","title":"Training Serving Skew","text":"<p>A common use case for observability is when your model is working well in training but poorly in production. Data scientists often find that the data their model was trained on is statistically different from the data they see in production. This discrepancy can be due to any number of factors: the sample size of your training set may not be enough to capture all possible conditions and edge cases; some of your features may be correlated; or there may be seasonal or event-driven variations in the data that weren't captured by your dataset.</p>"},{"location":"concepts/what_is_ml_observability/#how-to-ensure-that-ml-model-is-working-correctly","title":"How to ensure that ML model is working correctly?","text":"ML Monitoring Lifecycle <p>ML observability has to track the lifecycle of an ML model from its inception through training, validation and deployment. It encompasses a broad set of capabilities, including the ability to:</p>"},{"location":"concepts/what_is_ml_observability/#test-pre-production-validation","title":"Test &amp; Pre-Production Validation","text":"<p>In order to ensure that model behavior conforms to your expectations, you need to monitor the model\u2019s performance during Pre-Production validation. ML observability tools allow you to track a model\u2019s performance for each defined slice in the training data, so you can see how well it will generalize when deployed into production.</p>"},{"location":"concepts/what_is_ml_observability/#monitor-production-system","title":"Monitor Production System","text":"<p>When model is deployed to production, ML Observability keeps track of all of the input features and output predictions to provide proactive alerts. These alerts can be used as early warning signs of potential issues with the model. The user can also use these alerts to debug the model by checking whether any of these inputs have changed since they last checked, or if any of these outputs are not being predicted correctly.</p>"},{"location":"concepts/what_is_ml_observability/#root-cause-analysis","title":"Root Cause Analysis","text":"<p>When a model in production is failing to perform as expected, the first step toward a resolution is to understand what happened. This determination can be difficult because the model may have been trained and tested on different data than what it's operating on now, or it may have been trained with different hyperparameters than those being used now. In both cases, the network weights could have changed substantially from their training parameters, meaning that a new best-fit line wouldn't exist.</p> <p>With the help of Observability platform to monitor your models in production, you'll be able to determine exactly which distributions in input data, features, ground truth/proxy metrics have contributed to a change in the model\u2019s performance by combining your historical data with your model's current performance. The result of this analysis will let you pinpoint the cause of the problem and continue on to resolving it.</p>"},{"location":"introduction/introduction/","title":"Introduction to Waterdip","text":"<p>Waterdip is an Open Source ML Observability platform that allows ML Engineers and data scientists to observe their models' performance at scale.</p> <p>Waterdip allows you to monitor your model's performance in real-time. It also provides a dashboard that lets you track key metrics like accuracy, speed, cost, etc. The platform also gives you historical data about your model\u2019s performance so you can compare it against different timelines.</p>"},{"location":"introduction/introduction/#connect-anything-anywhere","title":"Connect anything, anywhere","text":"<p>Waterdip is an Open Platform  that works with your machine-learning infrastructure. We're model and platform agnostic, and 100% API-first.</p> <p>We make it easy to integrate with the tools of your choice; whether that's TensorFlow or Caffe2 or PyTorch or TensorFlow or any other framework you prefer.</p> <p></p>"},{"location":"introduction/quick_start/","title":"QuickStart","text":"<p>In this document, you will learn how to create a model, upload dataset and send prediction data to Waterdip.</p> <p>We are using the famous iris dataset, that is present in the scikit learn library to walk you through the various steps that are involved in getting you started and start monitoring your models.</p> <ul> <li>Import libraries and Download dataset</li> </ul> <pre><code>#Import all the libraries:\nimport requests\nimport uuid\nfrom sklearn import datasets\n</code></pre> <p>Download the Iris dataset from sklearn <pre><code>HOST = \"&lt;backend_host&gt; by default https://127.0.0.1:4422\"\niris = datasets.load_iris()\n</code></pre></p>"},{"location":"introduction/quick_start/#register-model","title":"Register Model","text":"<ul> <li>To register a new model for monitoring, register_model() function is used. It takes in the name of the model as parameter.</li> <li>The function returns the model id of the new model.</li> </ul> <pre><code>def register_model(model_name):\npost_url = f\"{HOST}/v1/model.register\"\ndata_dict = {\n\"model_name\": model_name\n}\nresponse = requests.post(url=post_url, json=data_dict)\nreturn response.json()['model_id']\n</code></pre>"},{"location":"introduction/quick_start/#register-model-version","title":"Register Model Version","text":"<ul> <li>This function registers a version for the existing model like version 1, version 2, etc. This increases reusability as same model can be used when there are few changes in the previous version and progress can be tracked.</li> <li>The function takes in model_id, data (for which the model is registered) as input and return model_version_id.</li> <li>Version_schema takes the names and data types of features and well as target (predictions) variables. Datatype can be NUMERICAL, CATEGORICAL or BOOLEAN. (Iris has only numerical feature values thus NUMERIC is hardcoded)</li> </ul> <pre><code>def register_model_version(data, model_id):\nurl = f\"{HOST}/v1/model.version.register\"\ndata_dict = {\n\"model_id\": str(model_id),\n\"model_version\": \"v1\",\n\"task_type\": \"MULTICLASS\",\n\"description\": \"This is iris dataset\",\n\"version_schema\": {\n\"features\": {x: \"NUMERIC\" for x in data.feature_names},\n\"predictions\": {x: \"NUMERIC\" for x in data.target_names}\n}\n}\nr = requests.post(url=url, json=data_dict)\nreturn r.json()['model_version_id']\n</code></pre>"},{"location":"introduction/quick_start/#log-training-dataset","title":"Log Training Dataset","text":"<ul> <li>Iris dataset(data) is passed as a parameter to the function log_training_data()</li> <li>Data_vals, target: a small part of dataset (for demo purpose)</li> <li>Data_dict: A dictionary that contains information about the dataset, I.e. model_version_id, and rows(events)</li> </ul> <p>Note: The structure of upload_dataset, I.e. names of the keys of this dictionary must not be changed. Once the data is uploaded, it cannot be appended, updated or modified. <pre><code>def log_training_data(data, model_version_id):\nurl = f\"{HOST}/v1/log.dataset\"\ndata_dict = {\n\"model_version_id\": str(model_version_id),\n\"environment\": \"TRAINING\"\n}\nrows = []\ndata_vals = data.data[:1]\ntarget = data.target[:1]\nfor doc, val in zip(data_vals, target):\nrow_dict = {\n\"features\": {},\n\"predictions\": {}\n}\nfor x, y in zip(data.feature_names, doc):\nrow_dict['features'][x] = y\nfor i, x in enumerate(data.target_names):\nif i == val:\nrow_dict['predictions'][x] = 1\nelse:\nrow_dict['predictions'][x] = 0\nrows.append(row_dict)\ndata_dict[\"rows\"] = rows\nrequests.post(url=url, json=data_dict)\n</code></pre></p>"},{"location":"introduction/quick_start/#log-events-and-actuals","title":"Log events and actuals","text":"<p>After we have uploaded the training dataset, we must also send the actuals data so that we can monitor the performance.</p> <p>The following code  shows how to log events/actuals. <pre><code>def log_event(data, model_version_id):\nurl = f\"{HOST}/v1/log.events\"\ndata_dict = {\n\"model_version_id\": str(model_version_id)\n}\nevents = []\ndata_vals = data.data[:1]\ntarget = data.target[:1]\nfor doc, val in zip(data_vals, target):\nrow_dict = {\n\"event_id\": str(uuid.uuid4()),\n\"features\": {},\n\"predictions\": {},\n\"actuals\": {}\n}\nfor x, y in zip(data.feature_names, doc):\nrow_dict['features'][x] = y\nfor i, x in enumerate(data.target_names):\nif i == val:\nrow_dict['predictions'][x] = 1\nrow_dict['actuals'][x] = 1\nelse:\nrow_dict['predictions'][x] = 0\nrow_dict['actuals'][x] = 0\nevents.append(row_dict)\ndata_dict[\"events\"] = events\nrequests.post(url=url, json=data_dict)\n</code></pre></p> <ul> <li>Log_event() takes model_version_id as parameter i.e. the version for which we want to log actuals</li> <li>Data_vals, target: a small part of dataset (for demo purpose)</li> <li>Data_dict contains model_version_id, and events dictionary that specifies the values for features, and corresponding targets and actuals.</li> </ul>"},{"location":"model_types/binary_classification/","title":"Binary Classification","text":""},{"location":"model_types/binary_classification/#data-example","title":"Data Example","text":"<p>Dataset for practicing classification -use NBA rookie stats to predict if player will last 5 years in league</p> <p>y = 0 if career years played &lt; 5 y = 1 if career years played &gt;= 5</p> <p>Data reference</p> <p></p>"},{"location":"model_types/binary_classification/#register-a-binary-classification-model","title":"Register a Binary Classification Model","text":"<p>Code snippet for registering Binary Classification Model</p>"},{"location":"model_types/multi_class_classification/","title":"Multi Class Classification","text":""},{"location":"sending_data/sending_live_events/","title":"Sending Live Events","text":"<p>One way to publish production data is by streaming data asynchronously, which allows for greater scalability.</p> <p>This process is straightforward, but it requires that each event be structured as a Python dictionary that maps field names to values.</p>"},{"location":"sending_data/sending_live_events/#event-structure","title":"Event Structure","text":""},{"location":"sending_data/sending_live_events/#event-dictionary","title":"Event Dictionary","text":"<p>Below is the structure of one prediction event. It consists of <code>features</code>, <code>predictions</code>, <code>event_id</code> and <code>actuals</code>.</p> <pre><code>event = {\n\"event_id\": \"&lt;PREDICTION_ID&gt;\",\n\"features\": {},\n\"predictions\": {},\n\"actuals\": {}\n}\n</code></pre>"},{"location":"sending_data/sending_live_events/#actuals","title":"Actuals","text":"<p>In some cases, your model will produce a prediction that can be evaluated based on real-world data.</p> <p>For example, if your model predicted that a client will buy term insurance, and one day later the client makes a purchase, then you can evaluate the accuracy of that prediction by comparing it to similar cases in which no purchase was made.</p>"},{"location":"sending_data/sending_live_events/#event-id","title":"Event ID","text":"<p>Each prediction must have a unique ID. This can be used later to log the actual value of the prediction. Event ID should be unique.</p> <p>If you do not care about actual logging send UUID4 as an <code>event_id</code>. For example: <code>str(uuid.uuid4())</code></p>"},{"location":"sending_data/sending_live_events/#sending-prediction-events","title":"Sending Prediction Events","text":"<p>To log the prediction event of your model, you can use the <code>/v1/log.events</code> API. User can send multiple events in one POST API.</p> <pre><code>import httpx\nimport uuid\nHOST = \"&lt;backend_host&gt; by default https://127.0.0.1:4422\"\nevents = [\n{\n\"event_id\": str(uuid.uuid4()),\n\"features\": {\n\"cap-shape\": \"x\",\n\"cap-surface\": \"s\",\n\"cap-color\": \"y\",\n\"bruises\": \"t\",\n\"odor\": \"l\",\n\"gill-attachment\": \"f\",\n\"gill-spacing\": \"c\"\n},\n\"predictions\": {\n\"class\": \"p\"\n}\n}\n]\nasync with httpx.AsyncClient() as client:\nresponse = await client.post(f\"{HOST}/v1/log.events\", data=events)\n</code></pre>"},{"location":"sending_data/uploading_baseline_dataset/","title":"Uploading Baseline Dataset","text":"<p>To monitor data integrity and drift issues in incoming production data, you need a baseline dataset with which to compare it.</p> <p>By default, the baseline data is the last 15 days moving window timeline of production data. User can change the baseline to pre-production data i.e. Training, Testing or Validation dataset. In order to do that user needs to upload the pre-production dataset.</p>"},{"location":"sending_data/uploading_baseline_dataset/#uploading-datasets","title":"Uploading Datasets","text":"<p>Info</p> <p>For every Model Version user can upload maximum 3 datasets, each for these three ENVIRONMENTS i.e. <code>TRAINING</code>, <code>TESTING</code> and <code>VALIDATION</code></p>"},{"location":"sending_data/uploading_baseline_dataset/#uploading-training-dataset","title":"Uploading Training Dataset","text":"<p>Dataset Reference</p> <p>For this example, Mushroom Dataset has been used.</p> <p>To log the training or test datasets of your model, you can use the <code>/v1/log.dataset</code> API.</p> <p>For example, if we have the following training set:</p> <pre><code>import pandas as pd\ntraining_features = pd.DataFrame({\n\"cap-shape\": [\"x\", \"x\", \"b\"],\n\"cap-surface\": [\"s\", \"s\", \"s\"],\n\"cap-color\": [\"n\", \"y\", \"w\"],\n\"bruises\": [\"t\", \"t\", \"t\"],\n\"odor\": [\"p\", \"a\", \"l\"],\n\"gill-attachment\": [\"f\", \"f\", \"f\"],\n\"gill-spacing\": [\"c\", \"c\", \"c\"],\n})\ntraining_predictions = pd.DataFrame({\n\"class\": [\"p\", \"e\", \"e\"],\n})\n</code></pre> <p>Then we can upload the training data using below process <pre><code>import requests\nHOST = \"&lt;backend_host&gt; by default https://127.0.0.1:4422\"\nMODEL_VERSION_ID = \"&lt;&lt;model_version_id&gt;&gt;\"\npredictions = training_predictions.to_dict(orient='index')\nrows = []\nfor index_no, features in training_features.to_dict(orient='index').items():\nrow = {\n\"features\": features,\n\"predictions\": training_predictions.iloc[index_no][\"class\"]\n}\nrows.append(row)\ndata_dict = {\n\"model_version_id\": MODEL_VERSION_ID,\n\"environment\": \"TRAINING\",\n\"rows\": rows\n}\nrequests.post(url=f\"{HOST}/v1/log.dataset\", json=data_dict)\n</code></pre></p>"},{"location":"sending_data/uploading_baseline_dataset/#uploading-testing-and-validation-dataset","title":"Uploading Testing and Validation Dataset","text":"<p>For uploading Testing or validation dataset you just need to change the environment variable</p> <p>For example, if we have to upload Testing dataset:</p> <pre><code>data_dict = {\n\"model_version_id\": MODEL_VERSION_ID,\n\"environment\": \"TESTING\",\n\"rows\": rows\n}\n</code></pre> <p>For example, if we have to upload Validation dataset:</p> <pre><code>data_dict = {\n\"model_version_id\": MODEL_VERSION_ID,\n\"environment\": \"VALIDATION\",\n\"rows\": rows\n}\n</code></pre>"},{"location":"support/contact/","title":"Contact","text":""},{"location":"support/contact/#github","title":"Github","text":"<p>Open an issue on GitHub to report bugs and ask questions.</p>"},{"location":"support/contact/#discord","title":"Discord","text":"<p>Please join our Discord Channel to chat and connect.</p>"},{"location":"support/contact/#email","title":"Email","text":"<p>Drop a mail at hi@waterdip.ai, and we will get back to you.</p>"}]}